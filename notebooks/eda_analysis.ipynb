{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 20:44:00,800 - INFO - Loading data from: ..\\data\\raw\\data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>ATL_OR_DR</th>\n",
       "      <th>CAMPAIGN_TYPE</th>\n",
       "      <th>CHANNEL</th>\n",
       "      <th>COST</th>\n",
       "      <th>FREE_TRIALS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/9/2023</td>\n",
       "      <td>DR - Direct Response</td>\n",
       "      <td>Title</td>\n",
       "      <td>paid social</td>\n",
       "      <td>7784.31</td>\n",
       "      <td>86401.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/29/2023</td>\n",
       "      <td>DR - Direct Response</td>\n",
       "      <td>Title</td>\n",
       "      <td>app</td>\n",
       "      <td>2474.31</td>\n",
       "      <td>2956.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/23/2023</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Title</td>\n",
       "      <td>paid social</td>\n",
       "      <td>10222.82</td>\n",
       "      <td>12513.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/23/2023</td>\n",
       "      <td>DR - Direct Response</td>\n",
       "      <td>Title</td>\n",
       "      <td>app</td>\n",
       "      <td>49631.87</td>\n",
       "      <td>17207.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/31/2022</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Title</td>\n",
       "      <td>paid social</td>\n",
       "      <td>2081.06</td>\n",
       "      <td>21758.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14103</th>\n",
       "      <td>2/23/2024</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Brand</td>\n",
       "      <td>bvod</td>\n",
       "      <td>1478.78</td>\n",
       "      <td>16941.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14104</th>\n",
       "      <td>4/21/2024</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Title</td>\n",
       "      <td>ooh</td>\n",
       "      <td>839.25</td>\n",
       "      <td>5727.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14105</th>\n",
       "      <td>12/13/2023</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Title</td>\n",
       "      <td>bvod</td>\n",
       "      <td>193796.71</td>\n",
       "      <td>34824.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14106</th>\n",
       "      <td>2/4/2023</td>\n",
       "      <td>ATL - Above The Line</td>\n",
       "      <td>Brand</td>\n",
       "      <td>paid social</td>\n",
       "      <td>84690.65</td>\n",
       "      <td>25140.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14107</th>\n",
       "      <td>2/17/2023</td>\n",
       "      <td>DR - Direct Response</td>\n",
       "      <td>Title</td>\n",
       "      <td>youtube</td>\n",
       "      <td>7385.46</td>\n",
       "      <td>17770.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14108 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      REPORT_DATE             ATL_OR_DR CAMPAIGN_TYPE      CHANNEL       COST  \\\n",
       "0        6/9/2023  DR - Direct Response         Title  paid social    7784.31   \n",
       "1       8/29/2023  DR - Direct Response         Title          app    2474.31   \n",
       "2       7/23/2023  ATL - Above The Line         Title  paid social   10222.82   \n",
       "3      12/23/2023  DR - Direct Response         Title          app   49631.87   \n",
       "4       8/31/2022  ATL - Above The Line         Title  paid social    2081.06   \n",
       "...           ...                   ...           ...          ...        ...   \n",
       "14103   2/23/2024  ATL - Above The Line         Brand         bvod    1478.78   \n",
       "14104   4/21/2024  ATL - Above The Line         Title          ooh     839.25   \n",
       "14105  12/13/2023  ATL - Above The Line         Title         bvod  193796.71   \n",
       "14106    2/4/2023  ATL - Above The Line         Brand  paid social   84690.65   \n",
       "14107   2/17/2023  DR - Direct Response         Title      youtube    7385.46   \n",
       "\n",
       "       FREE_TRIALS  \n",
       "0         86401.15  \n",
       "1          2956.74  \n",
       "2         12513.08  \n",
       "3         17207.04  \n",
       "4         21758.33  \n",
       "...            ...  \n",
       "14103     16941.67  \n",
       "14104      5727.03  \n",
       "14105     34824.51  \n",
       "14106     25140.77  \n",
       "14107     17770.26  \n",
       "\n",
       "[14108 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "import scipy.stats as stats\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# # Add src directory to the Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.df_data_attribute_analysis import categorize_columns\n",
    "from src.import_data import get_data, save_data\n",
    "\n",
    "df = get_data('data.csv')\n",
    "\n",
    "# Display a preview of the data\n",
    "df.rename(columns={'CAMPAIGN_TYPE ':'CAMPAIGN_TYPE', 'FREE TRIALS': 'FREE_TRIALS'}, inplace=True)\n",
    "nsize = df.shape[0]\n",
    "nsize\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(data, column, multiplier=1.5):\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the bounds for outliers with a tighter multiplier\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = remove_outliers_iqr(df, 'COST', 1.0)\n",
    "# df = remove_outliers_iqr(df, 'FREE_TRIALS', 1.0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Removed {round((nsize - df.shape[0]) / nsize * 100, 0)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Types Breakdown:\n",
    "    a. REPORT_DATE: Numerical | (Discrete -> Interval -> DateTime object)\n",
    "    b. ATL_OR_DR: Categorical | (Nominal  -> String)\n",
    "    c. CAMPAIGN_TYPE: Categorical | (Nominal -> String)\n",
    "    d. CHANNEL: Categorical | (Nominal -> String)\n",
    "    e. COST: Quantitative | (Continuous -> Ratio -> Float)\n",
    "    f. FREE TRAILS: Quantitative | ( Continuous -> Ratio -> Float)\n",
    "\n",
    "2. Data Questions/Concerns:\n",
    "    a. Is 'FREE TRAILS' a generated impression count from another model? How does a free trial\n",
    "    lead to continous values instead of discrete?\n",
    "\n",
    "=> Action Items:\n",
    "    a. Ensure proper data types across dataframe - DONE\n",
    "    b. Identify missing data gaps - DONE\n",
    "    c. Removing 0's based on EDA analysis and log transformation revealing 0 to be a major outlier. Also,\n",
    "        from a logical business standpoint, doesn't seem right for a campaign to have 0 cost  especailly\n",
    "        where some 0 cost campaigns generated higher than average free trials. - DONE\n",
    "'''\n",
    "colNames = df.columns\n",
    "categorical = colNames[1:4].values.tolist()\n",
    "quantitative = colNames[4::].values.tolist()\n",
    "\n",
    "# Included holidays and Super Bowl dates with the assumption that there are more campaigns being ran during these dates\n",
    "# Also assuming that holiday seasons have more cost associated with it but higher free trials being generated.\n",
    "df['REPORT_DATE'] = pd.to_datetime(df['REPORT_DATE'])\n",
    "us_holidays = holidays.US()\n",
    "super_bowl_dates = [\"2022-02-13\", \"2023-02-12\", \"2024-02-11\"]\n",
    "for date in super_bowl_dates:\n",
    "    us_holidays[date] = \"Super Bowl\"\n",
    "df['HOLIDAY_FLAG'] = df['REPORT_DATE'].apply(lambda x: x in us_holidays if pd.notnull(x) else False)\n",
    "df['LAG_2_HOLIDAY_FLAG'] = df['REPORT_DATE'].apply(lambda x: any((x + pd.Timedelta(days=offset)) in us_holidays for offset in range(1, 3)) if pd.notnull(x) else False)\n",
    "df['LEAD_2_HOLIDAY_FLAG'] = df['REPORT_DATE'].apply(lambda x: any((x - pd.Timedelta(days=offset)) in us_holidays for offset in range(1, 3)) if pd.notnull(x) else False)\n",
    "df['HOLIDAY_FLAG'] = df[['HOLIDAY_FLAG', 'LAG_2_HOLIDAY_FLAG', 'LAG_2_HOLIDAY_FLAG']].any(axis=1)\n",
    "df.drop(columns=['LAG_2_HOLIDAY_FLAG', 'LEAD_2_HOLIDAY_FLAG'], inplace=True)\n",
    "\n",
    "# Extract the month and season from the date to get better insights from metadata.\n",
    "month_to_season = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Winter\",\n",
    "    3: \"Spring\",\n",
    "    4: \"Spring\",\n",
    "    5: \"Spring\",\n",
    "    6: \"Summer\",\n",
    "    7: \"Summer\",\n",
    "    8: \"Summer\",\n",
    "    9: \"Fall\",\n",
    "    10: \"Fall\",\n",
    "    11: \"Fall\",\n",
    "    12: \"Winter\"\n",
    "}\n",
    "#df['SEASON'] = df['REPORT_DATE'].dt.month.map(month_to_season)\n",
    "df['MONTH'] = df['REPORT_DATE'].dt.month_name()\n",
    "\n",
    "# Combine categorical data for feature engineering to capture larger groups/trends:\n",
    "# Targeted 'CHANNEL' since that showed slight more relationship with free trial and\n",
    "# cost with other categorical variables.\n",
    "#df['CHANNEL_ATL_OR_DR'] = df['CHANNEL'] + '_' + df['ATL_OR_DR']\n",
    "#df['CHANNEL_CAMPAIGN_TYPE'] = df['CHANNEL'] + '_' + df['CAMPAIGN_TYPE']\n",
    "\n",
    "categorical.append('MONTH')#categorical.extend(['HOLIDAY_FLAG', 'MONTH'])\n",
    "\n",
    "def generate_combinations(categorical):\n",
    "    def backtrack(start, path):\n",
    "        # Append the current path (combination) to the result\n",
    "        result.append(path[:])\n",
    "\n",
    "        # Explore further combinations by including elements one by one\n",
    "        for i in range(start, len(categorical)):\n",
    "            # Include the current element\n",
    "            path.append(categorical[i])\n",
    "\n",
    "            # Recurse with the updated path\n",
    "            backtrack(i + 1, path)\n",
    "\n",
    "            # Backtrack: remove the last element to explore other combinations\n",
    "            path.pop()\n",
    "\n",
    "    # Result to store all combinations\n",
    "    result = []\n",
    "    # Start the DFS/backtracking\n",
    "    backtrack(0, [])\n",
    "    return result\n",
    "\n",
    "combinations = generate_combinations(categorical)\n",
    "\n",
    "# Print all combinations\n",
    "for combo in combinations:\n",
    "    print(combo)\n",
    "\n",
    "for combo in combinations:\n",
    "    # Ensure the combination has unique column names\n",
    "    if len(combo) != len(set(combo)):\n",
    "        continue  # Skip combinations with duplicate column names\n",
    "\n",
    "    # Generate the column name\n",
    "    column_name = '_'.join(combo)\n",
    "\n",
    "    # Skip if the combination is empty, already exists, or is single-element\n",
    "    if not combo or column_name in df.columns or len(combo) == 1:\n",
    "        continue\n",
    "\n",
    "    # Simplified row-by-row logic to create the new column\n",
    "    new_column = []\n",
    "    for index in range(len(df)):\n",
    "        concatenated_value = \"\"\n",
    "        for col in combo:\n",
    "            if concatenated_value == \"\":\n",
    "                concatenated_value = str(df.loc[index, col])\n",
    "            else:\n",
    "                concatenated_value += \"_\" + str(df.loc[index, col])\n",
    "        new_column.append(concatenated_value)\n",
    "\n",
    "    # Add the new column to the DataFrame\n",
    "    df[column_name] = new_column\n",
    "\n",
    "# Combine numerical data for feature engineering:\n",
    "# Created KPI metric efficency cost per free trial generated\n",
    "df['COST_PER_FREE_TRIALS'] = df['COST'] / df['FREE_TRIALS']\n",
    "df['COST_BY_FREE_TRIALS'] = df['COST'] * df['FREE_TRIALS']\n",
    "quantitative.extend(['COST_PER_FREE_TRIALS','COST_BY_FREE_TRIALS'])\n",
    "\n",
    "# Results\n",
    "print(f'Categorical | {len(categorical)}: {categorical}\\nQuantitative | {len(quantitative)}: {quantitative} \\n')\n",
    "print(f'Table Dimension: {df.shape} \\n')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis on Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single Variation Continuous Analysis via Histogram/Boxplots:\n",
    "\n",
    "1. All visuals shows a right skewed distribution with outliers on the right side. So, the mean will be greater than the median.\n",
    "\n",
    "2. All visuals show data points are clustered around the lower ends and extreme outliers causing the positive skewness.\n",
    "\n",
    "3. The max values for all data, (COST = 3M, FREE_TRIALS = 2M), are significantly larger than the  (IQR) and mean values,\n",
    "validating the outliers. More analysis is needed to determine if these should be removed or kept.\n",
    "\n",
    "4. With such skewness, median would be a more viable measure of central tendency than the mean.\n",
    "\n",
    "5. Most of the cluster of COST is around 0 which is surprising because I wouldn't have guessed the majority of campaigns to incur 0 expenses.\n",
    "Is 0 a placeholder for missing data?\n",
    "\n",
    "6.  All visuals violates Linear Regression's assumption of normality. Need to log transform this feature to normalize the data and reduce outlier\n",
    "impact\n",
    "\n",
    "7. FREE TRIALS has a broader distribution of values giving incentives to companies/clients to really optimize their campaign stratgey.\n",
    "This also signals that some campaigns can also genterate minimal free trials.\n",
    "\n",
    "8. COST is more concentrated near zero. This tell us that it's either relatively inexpensive and consistent when it comes the cost of runnning a\n",
    "campaign or companies aren't allocating enough to run campaigns.\n",
    "\n",
    "9. The lowest free trial generated for a campaign is 500. Interesting it's not 0 unlike COST. Is 500 suppose to repersent 0?\n",
    "\n",
    "10. A lot of outliers are visible in boxplot visuals. The fact that we have a lot of points clustered around the upper whisker makes me thinks\n",
    "there is an actual reason behind such concentration of data points for cost and free trials. (IE: Segmentation Analysis)\n",
    "\n",
    "11. COST/FREE TRIALS are generally on the lower end. We don't see a lot of campaigns being exorbitantly expensive or generating a lot of free trials.\n",
    "    a. Campaigns generally cost $5,000-$10,000\n",
    "    b. campaings genearlly generate 20,000-40,000 free trials\n",
    "    c. having smaller values means more efficency.\n",
    "\n",
    "12. What separates the pack of companies/clients that are spending exorbitant amount on campaaigns? What/why are they doing differently?\n",
    "\n",
    "=> Action Items:\n",
    "    a. Log Transformation on 'COST' and 'FREE TRIALS'. - DONE\n",
    "    b. Single Variation Continuous Analysis using boxplots - DONE\n",
    "    c. Covaritation Continuous Analysis between 'COST' and 'FREE TRIALS' using a scatterplot. - DONE\n",
    "    d. Validate assumption that cost and free trials doesn't have a strong linear relationship. - DONE\n",
    "    e. Create low, medium, high spenders as binning categories for COST. Can be usefulr for customer\n",
    "    segmentation analysis\n",
    "'''\n",
    "# Create a grid layout\n",
    "num_cols = len(quantitative)\n",
    "ncols = 4  # Set the number of columns per row\n",
    "nrows = (num_cols // ncols) + (num_cols % ncols > 0)  # Adjust rows dynamically based on total columns\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))  # Wider figure for 4 columns\n",
    "\n",
    "# Plot each quantitative column\n",
    "for i, quant_col in enumerate(quantitative):\n",
    "    sns.histplot(df[quant_col], kde=True, bins=118, ax=axes[i]) #118 bins | bin size ~= SqRoot(n):\n",
    "    axes[i].set_title(f'Distribution of {quant_col}')\n",
    "    axes[i].set_xlabel(quant_col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_cols = len(quantitative)\n",
    "ncols = 4  # Number of columns per row\n",
    "nrows = (num_cols // ncols) + (num_cols % ncols > 0)  # Adjust rows dynamically\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "\n",
    "# Plot each quantitative column\n",
    "for i, col in enumerate(quantitative):\n",
    "    sns.boxplot(x=col, data=df, ax=axes[i])  # Removed palette argument\n",
    "    axes[i].set_title(f'Box plot of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for boxplots without outliers\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "axes = axes.flatten() if nrows * ncols > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(quantitative):\n",
    "    sns.boxplot(x=col, data=df, showfliers=False, ax=axes[i])  # Removed palette argument\n",
    "    axes[i].set_title(f'Box plot of {col} (Without Outliers)')\n",
    "    axes[i].set_xlabel(col)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df[quantitative].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(2)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(14)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single Variation LOG-Continuous Analysis via Histogram/Boxplots:\n",
    "\n",
    "1. The log transformation has normalized the data and reduced the impact of outliers. The distribution is now more symmetrical.\n",
    "\n",
    "2. Cost Data: 68% of all campaigns to fall within one standard deviation of this range $402 -$22,025; the mean/median is $3,000.\n",
    "\n",
    "3. Free Trials: 68% of all campaigns to fall within one standard deviation of this range 3,000 - 160,000; the mean/median is 22,000.\n",
    "\n",
    "4. Free Trials has a broader distribution of values than Cost, which validates visuals seen already. You can spend a fixed cost but\n",
    "have higher variance in free trials generated. This tells us cost is not a good indicator of free trials generated and that there are\n",
    "more factors at play.\n",
    "\n",
    "5. The log boxplots still show outliers but now on both tails except for LOG_COST_PER_TRIALS. Left tail might not offer much savings opportunities,\n",
    "but the right tail might provide opportunities to reduce expenses or look into a more cost savings campaign strategy. The fact that there are low\n",
    "variances in both tails of log cost and log free trial this could be some clustering behavior.\n",
    "\n",
    "6. Segmenting data into 2 groups for cost and free trials would be a good baseline for segmentation before running clustering algorithms. 2 groups\n",
    "refering to your typical spenders and then your abnormal spenders. Can also use 3 groups for low, medium, high costing campaigns and free trials\n",
    "generated.\n",
    "\n",
    "7. LOG_COST_PER_TRIALS has less variance than the other log-transformed variables. This could be due to the fact many comapnies adopt similar\n",
    "startegies/themes/objectives regarding striving for efficent cost per free trial. However, we do have a bucnh of exterme outliers that are\n",
    "inefficent. Thiscould liely be either misallocation of resources/stategy, or these companies can afford to spend more on campaigns.\n",
    "\n",
    "8. The fact that there are so many present outliers on oppsoite whiskers for most visuals makes me think these are more than just outliers and\n",
    "instead suggests distinct business segments/behavioral clusters and/or domain behavior.\n",
    "\n",
    "=>. Action Items:\n",
    "    a. Add cateogircal data to the scatterplot to see if there are any relationships between the two.\n",
    "    b. Filter data on 0's in 'LOG_COST' for more analysis.\n",
    "    c. Filter data on high 'LOG_COST' with low 'LOG_FREE TRIALS' for more analysis.\n",
    "    d. Filter data on low 'LOG_COST' and high 'LOG_FREE TRIALS' for more analysis.\n",
    "    e. Deal with outliers - DONE\n",
    "\n",
    "'''\n",
    "num_cols = len(quantitative)\n",
    "ncols = 4  # Number of columns per row\n",
    "nrows = (num_cols // ncols) + (num_cols % ncols > 0)  # Dynamically adjust rows\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "\n",
    "# Flatten axes for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for log-transformed variables\n",
    "for i, quant_col in enumerate(quantitative):\n",
    "    log_transformed = np.log1p(df[quant_col])  # Log-transform\n",
    "    df[f'LOG_{quant_col}'] = log_transformed  # Add to dataframe\n",
    "    sns.histplot(log_transformed, kde=True, bins=118, ax=axes[i])\n",
    "    axes[i].set_title(f'Log Distribution of {quant_col}')\n",
    "    axes[i].set_xlabel(f'LOG_{quant_col}')\n",
    "\n",
    "# Remove unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_cols = len(quantitative)\n",
    "ncols = 4\n",
    "nrows = (num_cols // ncols) + (num_cols % ncols > 0)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(quantitative):\n",
    "    sns.boxplot(x=df[f'LOG_{col}'], data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Box plot of LOG_{col}')\n",
    "    axes[i].set_xlabel(f'LOG_{col}')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "for i, col in enumerate(quantitative):\n",
    "    sns.boxplot(x=df[f'LOG_{col}'], data=df, showfliers=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Box plot of LOG_{col} (Without Outliers)')\n",
    "    axes[i].set_xlabel(f'LOG_{col}')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df[quantitative].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiVariate Analysis on Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Covariation Analysis using Scatterplot:\n",
    "\n",
    "1. LOG_COST and LOG_FREE_TRIALS doesn't show a strong linear relationship. There is some positive relationship but we also see diminishing\n",
    "returns and/or increasing likelihood of a campaign not generating a typical \"linearly\" free trial count aka risk. Likewise, we see a bunch of campaigns\n",
    "that had minimal costs and yet generated a lot of free trials. This could be really efficent campaign strategy or incorrect data misrepresentation.\n",
    "Regardless, somewaht a linear relationship with diminishing returns past a cost threshold.\n",
    "\n",
    "2. We see a a lot of non-linear relationships. We can either perform more feature enginering like square root or taking the polynomial or we can use a model to capture\n",
    "relationships beyond linearaity. Overall, no strong evidence of any visuls with any indication of any linear relationship.\n",
    "\n",
    "3. For campaigns that require minimal cost, there's a wide spread of LOG_COST_PER_FREE_TRIALS signalying high variability in efficiency at lower spending levels. This could be\n",
    "the \"gold standard\" comapnies that are succesfull in buidling their brand/lyoalty and customer base and leveraging that to generate free trials at a high efficeint clip (high efficeny).\n",
    "This is a \"sweet spot\" because even for campaigns that generated minimal free trials, at least your cost was small too. Therefore, less risk is associated while low cost and can have high\n",
    "ROI while increasing cost on a campaign drastically increases risk of low free trail genreated.\n",
    "\n",
    "=>. Action Items:\n",
    "    a. Add cateogircal data to the scatterplot to see if there are any relationships between the two. - DONE\n",
    "    b. Filter data on 0's in 'LOG_COST' for more analysis. - DONE\n",
    "    c. Filter data on high 'LOG_COST' with low 'LOG_FREE TRIALS' for more analysis.\n",
    "    d. Filter data on low 'LOG_COST' and high 'LOG_FREE TRIALS' for more analysis.\n",
    "'''\n",
    "totalColumns = set(df.columns)\n",
    "totalColumns.remove('REPORT_DATE')\n",
    "\n",
    "# quantitative=list(set(totalColumns) - set(categorical))\n",
    "# quantitative\n",
    "logs = [f'LOG_{quant_name}' for quant_name in quantitative]\n",
    "quantitative.extend(logs)\n",
    "#LOG_COST_PER_FREE_TRIALS\tLOG_COST_BY_FREE_TRIALS\n",
    "sns.pairplot(df[quantitative], diag_kind='kde', height=2.5)\n",
    "plt.suptitle('Pairplot of Quantitative Variables', y=1.02)  # Title for the entire plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hashmap = categorize_columns(df)\n",
    "data_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_hashmap['categorical']= ['ATL_OR_DR',\n",
    "#   'CAMPAIGN_TYPE',\n",
    "#   'CHANNEL',\n",
    "#   'HOLIDAY_FLAG',\n",
    "#   'MONTH','ATL_OR_DR_CHANNEL','CAMPAIGN_TYPE_CHANNEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_hashmap['numerical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pearson Correlation using Heatmap\n",
    "\n",
    "1. Most relationships show weak linaer relationships which is validated in the scatterplot. Since we are predicting FREE_TRIALS,\n",
    "the heatmap indicates that using a linear model might not be the most effective model to use.\n",
    "\n",
    "2. Some hot spots are misleading bc they are dervied from feature engineering from previous steps.\n",
    "'''\n",
    "# data_hashmap = categorize_columns(df)\n",
    "# data_hashmap\n",
    "\n",
    "encoded_df = pd.get_dummies(df[data_hashmap['categorical']], drop_first=False)  # Drop first to avoid multicollinearity\n",
    "# label_encoded_df = df[data_hashmap['categorical']].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Combine encoded variables with quantitative columns\n",
    "matrix_df = pd.concat([df[data_hashmap['numerical']], encoded_df], axis=1)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(matrix_df.corr(method='spearman'), annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "plt.xticks(fontsize=8, rotation=90)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# matrix_df = pd.concat([df[data_hashmap['numerical']], encoded_df], axis=1)\n",
    "# sns.heatmap(matrix_df.corr(), annot=True, cmap=\"coolwarm\", annot_kws={\"size\": 5})\n",
    "# plt.xticks(fontsize=6, rotation=90)\n",
    "# plt.yticks(fontsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Correlation Matrix Before applying filters, check the full correlation matrix:\n",
    "# print(matrix_df.corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr = matrix_df.corr()\n",
    "\n",
    "# Apply stricter filtering\n",
    "threshold = 0.75\n",
    "filtered_corr = corr.where((corr.abs() > threshold) & (corr != 1.0))\n",
    "\n",
    "# Drop rows/columns that are entirely NaN\n",
    "filtered_corr = filtered_corr.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "\n",
    "\n",
    "top_features = filtered_corr.abs().sum().sort_values(ascending=False).index[:20]\n",
    "reduced_corr = filtered_corr.loc[top_features, top_features]\n",
    "# Check if filtered_corr is empty\n",
    "if filtered_corr.empty:\n",
    "    print(\"No correlations found above the threshold.\")\n",
    "else:\n",
    "    # Dynamically adjust plot size with robust logic\n",
    "    # figsize = (max(10, min(len(reduced_corr) * 0.5, 30)),\n",
    "    #            max(10, min(len(reduced_corr) * 0.5, 30)))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(20,20))\n",
    "    #sns.heatmap(filtered_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, cbar=True)\n",
    "    sns.heatmap(\n",
    "    reduced_corr,\n",
    "    annot=True, #reduced_corr.abs() > 0.85,  # Annotate strong correlations only\n",
    "    fmt=\".2f\",  # Format annotations\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar=True,\n",
    ")\n",
    "    plt.title(f\"Highly Correlated Features (|corr| > {threshold})\", fontsize=16)\n",
    "    plt.xticks(fontsize=10, rotation=90, ha='right')\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Filtered Correlation Check the shape of filtered_corr after filtering:\n",
    "print(filtered_corr.shape)\n",
    "print(filtered_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "# import seaborn as sns\n",
    "\n",
    "# corr = matrix_df.corr()\n",
    "# linkage_matrix = linkage(corr, method='ward')\n",
    "\n",
    "# sns.clustermap(\n",
    "#     corr,\n",
    "#     cmap=\"coolwarm\",\n",
    "#     linewidths=0.5,\n",
    "#     figsize=(15, 15),\n",
    "#     annot=False,\n",
    "#     row_cluster=True,\n",
    "#     col_cluster=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis on Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cats = list(totalColumns - set(quantitative))\n",
    "# categorical = (new_cats)\n",
    "# categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single Variation Categorical Analysis using Barchart:\n",
    "\n",
    "1. ATL is a more popular type of campaign over DR. This aligns with your typical business marketing lifecycle where\n",
    "companies/customers continuosly build and reinforce their brand over time. This is also apparent in the campaign types:\n",
    "title and brand are more popular while launch campaigns are less frequent. This could also relate to the fact that there\n",
    "is a high barriers of entry for launching new products or entering new market regions associated with cost. Promotional\n",
    "campaigns do not exist.\n",
    "\n",
    "2. Paid Social and other digitial media devices are the most popular over your traditional media devices like TV, radio, and print.\n",
    "This isn't to infer that these traditional media devices are not effective, but rather the shift in the digital age. Reduce\n",
    "spending on traditional media devices and reallocating resources to digital media devices could be a good strategy but you might\n",
    "lose out on niche customers/target audience that can only be reached a certain way.\n",
    "\n",
    "3. Fall/Winter show the highest number of campaigns being aired. This could be companies/customers wanting to capitalize on the\n",
    "holiday season. Specifically December shows the highest number of campaigns being aired followed by October and November(Black Friday/\n",
    "Cyber Monday?)\n",
    "\n",
    "=> Action Items:\n",
    "    a. Covariation Analysis between Categorical & Continuous' using Scatterplot and hue\n",
    "    b. Covariation Analysis between categorical and continuous using boxplot and barcharts\n",
    "'''\n",
    "\n",
    "top_n = 15  # Define the top N categories to display\n",
    "\n",
    "# Calculate the number of rows and columns dynamically based on the number of categorical variables\n",
    "num_cols = len(categorical)\n",
    "ncols = 4  # Number of columns per row\n",
    "nrows = (num_cols // ncols) + (num_cols % ncols > 0)  # Dynamically adjust rows\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "\n",
    "# Flatten axes for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot horizontal bar plots for each categorical variable\n",
    "for i, cat in enumerate(categorical):\n",
    "    # Get the top N categories and their counts\n",
    "    top_categories = df[cat].value_counts().head(top_n)\n",
    "    sns.barplot(y=top_categories.index, x=top_categories.values, ax=axes[i])\n",
    "\n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'Top {top_n} {cat} Categories')\n",
    "    axes[i].set_ylabel(cat)\n",
    "    axes[i].set_xlabel('Count')\n",
    "    axes[i].tick_params(axis='y', rotation=0)  # Keep y-axis labels horizontal for readability\n",
    "\n",
    "# Remove unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# num_cols = len(categorical)\n",
    "# ncols = 4  # Number of columns per row\n",
    "# nrows = (num_cols // ncols) + (num_cols % ncols > 0)  # Dynamically adjust rows\n",
    "# fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\n",
    "\n",
    "# # Flatten axes for iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Plot bar plots for each categorical variable\n",
    "# for i, cat in enumerate(categorical):\n",
    "#     sns.countplot(x=cat, data=df, ax=axes[i])\n",
    "#     axes[i].set_title(f'Bar plot of {cat}')\n",
    "#     axes[i].set_xlabel(cat)\n",
    "#     axes[i].set_ylabel('Count')\n",
    "#     axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# # Remove unused subplots\n",
    "# for j in range(i + 1, len(axes)):\n",
    "#     fig.delaxes(axes[j])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Covariation Analysis between Categorical & Continuous using boxplot and barcharts:\n",
    "\n",
    "1. ATL tend to have a slight higher cost associated with it than DR.\n",
    "\n",
    "2. Different channels have a higher/lower cost associated with running a campaign. This\n",
    "could be one of the driving factors in campaign strategies.\n",
    "\n",
    "3. It cost more to run on holidays than non-holidays. This can be an area of opportunity\n",
    "to see if spending more money to run on holidays is worth it or not regarding free trials\n",
    "generated.\n",
    "\n",
    "4. Spring tends to be the cheapest season to run a campaign. Perhaps leeast amount of holidays?\n",
    "\n",
    "5.\n",
    "'''\n",
    "''''''\n",
    "for category in categorical:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Create side-by-side plots\n",
    "\n",
    "    # Box Plot\n",
    "    sns.boxplot(x=category, y='COST', data=df, ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of COST by {category}')\n",
    "    axes[0].set_xlabel(category)\n",
    "    axes[0].set_ylabel('COST')\n",
    "    axes[0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Bar Chart (Mean)\n",
    "    mean_bar_data = df.groupby(category)['COST'].mean().reset_index()\n",
    "    sns.barplot(x=category, y='COST', data=mean_bar_data, ax=axes[1])\n",
    "    axes[1].set_title(f'Bar Chart of Mean COST by {category}')\n",
    "    axes[1].set_xlabel(category)\n",
    "    axes[1].set_ylabel('Mean COST')\n",
    "    axes[1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Bar Chart (Median)\n",
    "    median_bar_data = df.groupby(category)['COST'].median().reset_index()\n",
    "    sns.barplot(x=category, y='COST', data=median_bar_data, ax=axes[2])\n",
    "    axes[2].set_title(f'Bar Chart of Median COST by {category}')\n",
    "    axes[2].set_xlabel(category)\n",
    "    axes[2].set_ylabel('Median COST')\n",
    "    axes[2].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Covariation Analysis between Categorical & Continuous using boxplot and barcharts:\n",
    "\n",
    "1. ATL/DR doesn't show much of a difference in free trials generated despite ATL/DR being the more\n",
    "popular choice. CAMPAIGN_TYPE doesn't show much of a difference either.\n",
    "\n",
    "2. Similarly to cost, different channels have a higher/lower free trials generated. This could be\n",
    "an influential factor in campaign strategy.\n",
    "\n",
    "3. Free Trials don't generate higher clicks on holidays than non-holidays. If so, since we know it\n",
    "cost more to run on holidays, it's better for customers/clients to not run campaigns on holidays.\n",
    "One factor could be more competition and/or people are spending it with each other.\n",
    "\n",
    "4. Seasons/months don't show much of a difference in free trials generated.\n",
    "'''\n",
    "for category in categorical:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Create side-by-side plots\n",
    "\n",
    "    # Box Plot\n",
    "    sns.boxplot(x=category, y='FREE_TRIALS', data=df, ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of FREE_TRIALS by {category}')\n",
    "    axes[0].set_xlabel(category)\n",
    "    axes[0].set_ylabel('FREE_TRIALS')\n",
    "    axes[0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Bar Chart (Mean)\n",
    "    mean_bar_data = df.groupby(category)['FREE_TRIALS'].mean().reset_index()\n",
    "    sns.barplot(x=category, y='FREE_TRIALS', data=mean_bar_data, ax=axes[1])\n",
    "    axes[1].set_title(f'Bar Chart of Mean FREE_TRIALS by {category}')\n",
    "    axes[1].set_xlabel(category)\n",
    "    axes[1].set_ylabel('Mean FREE_TRIALS')\n",
    "    axes[1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Bar Chart (Median)\n",
    "    median_bar_data = df.groupby(category)['FREE_TRIALS'].median().reset_index()\n",
    "    sns.barplot(x=category, y='FREE_TRIALS', data=median_bar_data, ax=axes[2])\n",
    "    axes[2].set_title(f'Bar Chart of Median FREE_TRIALS by {category}')\n",
    "    axes[2].set_xlabel(category)\n",
    "    axes[2].set_ylabel('Median FREE_TRIALS')\n",
    "    axes[2].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. When campaign spending is low, thereâ€™s higher variability in the number of free trials generated. For all visuals, as\n",
    "spending increases (especially in paid social campaigns), thereâ€™s a diminishing return in the number of free trials generated.\n",
    "Beyond a certain threshold (around the midpoint in the data), higher spending could lead to higher risks by the form of\n",
    "fewer free trials.\n",
    "\n",
    "2. Lot of campaigns that run on Pmax incur high expesnse. After looking into Pmax, these might involve higher upfront costs that decrease over time,\n",
    "although current data lacks appropriate metadata since unsure when contracts were signed. Meanwhile, Digital Audio campaigns have a lower cost.\n",
    "'''\n",
    "values = list(df['CHANNEL'].unique())\n",
    "for val in values:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create side-by-side plots\n",
    "\n",
    "    data = df[df['CHANNEL'] == val]  # Filter the data for the current value\n",
    "\n",
    "    # First scatter plot\n",
    "    sns.scatterplot(data=data, x='COST', y='FREE_TRIALS', ax=axes[0])\n",
    "    axes[0].set_title(f'{val} - COST vs FREE_TRIALS')\n",
    "    axes[0].set_xlabel('COST')\n",
    "    axes[0].set_ylabel('FREE_TRIALS')\n",
    "\n",
    "    # Second scatter plot\n",
    "    sns.scatterplot(data=data, x='LOG_COST', y='LOG_FREE_TRIALS', ax=axes[1])\n",
    "    axes[1].set_title(f'{val} - LOG_COST vs LOG_FREE_TRIALS')\n",
    "    axes[1].set_xlabel('LOG_COST')\n",
    "    axes[1].set_ylabel('LOG_FREE_TRIALS')\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = list(set(df.columns) - set(quantitative))\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical.remove('HOLIDAY_FLAG')\n",
    "categorical.remove('ATL_OR_DR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kruskal-Wallis H test is the equivalent to ANOVA but uses medians instead. ANOVA assumes data is normally distributed.\n",
    "'''\n",
    "\n",
    "from scipy import stats\n",
    "# Initialize an empty list to store results\n",
    "anova_results = []\n",
    "\n",
    "# Loop through each categorical and quantitative combination\n",
    "for category_dimension in categorical:\n",
    "    for quant in quantitative:\n",
    "        # Step 1: Get unique categories\n",
    "        categories = df[category_dimension].unique()\n",
    "\n",
    "        # Step 2: Group data by category\n",
    "        grouped_data = {}\n",
    "        for category in categories:\n",
    "            grouped_data[category] = df[df[category_dimension] == category][quant].values\n",
    "\n",
    "        # Step 3: Convert grouped data to a list of arrays\n",
    "        grouped_data_list = list(grouped_data.values())\n",
    "\n",
    "        # Step 4: Perform one-way ANOVA\n",
    "        try:\n",
    "            anova_result = stats.f_oneway(*grouped_data_list)\n",
    "            # Append results to the list\n",
    "            anova_results.append({\n",
    "                \"Categorical Variable\": category_dimension,\n",
    "                \"Quantitative Variable\": quant,\n",
    "                \"F-statistic\": anova_result.statistic,\n",
    "                \"p-value\": anova_result.pvalue\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            # Handle errors and append to the results\n",
    "            anova_results.append({\n",
    "                \"Categorical Variable\": category_dimension,\n",
    "                \"Quantitative Variable\": quant,\n",
    "                \"F-statistic\": None,\n",
    "                \"p-value\": None,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "anova_results_df = pd.DataFrame(anova_results)\n",
    "anova_results_df.sort_values('p-value', inplace=True, ascending=True)\n",
    "anova_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = anova_results_df[\n",
    "#     (anova_results_df['p-value'] <= 0.06) &\n",
    "#     (\n",
    "\n",
    "#         (anova_results_df['Quantitative Variable'] == 'LOG_COST') |\n",
    "#         (anova_results_df['Quantitative Variable'] == 'LOG_FREE_TRIALS')\n",
    "#     )\n",
    "# ]\n",
    "# filtered_df\n",
    "anova_results_df[(anova_results_df['p-value'] <=.06)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMPAIGN_TYPE_CHANNEL_TO_COST_MEAN_hashmap = df.groupby('CAMPAIGN_TYPE_CHANNEL')['COST'].mean().to_dict()\n",
    "CAMPAIGN_TYPE_CHANNEL_TO_COST_MEDIAN_hashmap = df.groupby('CAMPAIGN_TYPE_CHANNEL')['COST'].median().to_dict()\n",
    "\n",
    "#\n",
    "\n",
    "MONTH_TO_COST_MEAN_hashmap = df.groupby('MONTH')['COST'].mean().to_dict()\n",
    "MONTH_TO_COST_MEDIAN_hashmap = df.groupby('MONTH')['COST'].median().to_dict()\n",
    "\n",
    "\n",
    "MONTH_TO_FREE_TRIALS_MEAN_hashmap = df.groupby('MONTH')['FREE_TRIALS'].mean().to_dict()\n",
    "MONTH_TO_FREE_TRIALS_MEDIAN_hashmap = df.groupby('MONTH')['FREE_TRIALS'].median().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG_COST_MONTH'] = df['MONTH'].map(MONTH_TO_COST_MEAN_hashmap)\n",
    "df['MEDIAN_COST_MONTH'] = df['MONTH'].map(MONTH_TO_COST_MEDIAN_hashmap)\n",
    "#\n",
    "\n",
    "df['AVG_COST_CAMPAIGN_TYPE_CHANNEL'] = df['CAMPAIGN_TYPE_CHANNEL'].map(CAMPAIGN_TYPE_CHANNEL_TO_COST_MEAN_hashmap)\n",
    "df['MEDIAN_COST_CAMPAIGN_TYPE_CHANNEL'] = df['CAMPAIGN_TYPE_CHANNEL'].map(CAMPAIGN_TYPE_CHANNEL_TO_COST_MEDIAN_hashmap)\n",
    "\n",
    "\n",
    "df['AVG_FREE_TRIALS_MONTH'] = df['MONTH'].map(MONTH_TO_FREE_TRIALS_MEAN_hashmap)\n",
    "df['MEDIAN_FREE_TRIALS_MONTH'] = df['MONTH'].map(MONTH_TO_FREE_TRIALS_MEDIAN_hashmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# results = pairwise_tukeyhsd(endog=df['FREE_TRIALS'], groups=df['MONTH'])\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the COST_PER_FREE_TRIALS by the ATL_OR_DR categories\n",
    "grouped_data = df.groupby('MONTH')['COST'].apply(list)\n",
    "\n",
    "# Perform a one-way ANOVA test\n",
    "anova_result = f_oneway(*grouped_data)\n",
    "\n",
    "# Display the ANOVA test results\n",
    "print(\"F-statistic:\", anova_result.statistic)\n",
    "print(\"p-value:\", anova_result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # categorical = ['ATL_OR_DR', 'HOLIDAY_FLAG']\n",
    "# from scipy.stats import ttest_1samp\n",
    "\n",
    "# # Group the COST data by the MONTH categories\n",
    "# grouped_data = df.groupby('MONTH')['FREE_TRIALS'].apply(list)\n",
    "\n",
    "# # Define the population mean to compare against\n",
    "# population_mean = 50  # Replace with your desired mean value\n",
    "\n",
    "# # Perform a one-sample t-test for each group\n",
    "# for month, data in grouped_data.items():\n",
    "#     t_test_result = ttest_1samp(data, population_mean)\n",
    "#     print(f\"Month: {month}\")\n",
    "#     print(f\"T-statistic: {t_test_result.statistic}\")\n",
    "#     print(f\"P-value: {t_test_result.pvalue}\")\n",
    "#     print()\n",
    "\n",
    "# # # Define the population mean to compare against\n",
    "# # population_mean = 50  # Replace with the desired mean value\n",
    "\n",
    "# # # Initialize an empty list to store results\n",
    "# # t_test_results = []\n",
    "\n",
    "# # # Loop through each categorical and quantitative combination\n",
    "# # for category_dimension in categorical:\n",
    "# #     for quant in quantitative:\n",
    "# #         # Step 1: Get unique categories\n",
    "# #         categories = df[category_dimension].unique()\n",
    "\n",
    "# #         # Step 2: Group data by category\n",
    "# #         grouped_data = {}\n",
    "# #         for category in categories:\n",
    "# #             grouped_data[category] = df[df[category_dimension] == category][quant].values\n",
    "\n",
    "# #         # Step 3: Perform one-sample t-test for each category\n",
    "# #         for category, values in grouped_data.items():\n",
    "# #             try:\n",
    "# #                 # Perform one-sample t-test\n",
    "# #                 t_test_result = stats.ttest_1samp(values, population_mean)\n",
    "\n",
    "# #                 # Append results to the list\n",
    "# #                 t_test_results.append({\n",
    "# #                     \"Categorical Variable\": category_dimension,\n",
    "# #                     \"Category\": category,\n",
    "# #                     \"Quantitative Variable\": quant,\n",
    "# #                     \"T-statistic\": t_test_result.statistic,\n",
    "# #                     \"p-value\": t_test_result.pvalue,\n",
    "# #                     \"Mean\": values.mean()\n",
    "# #                 })\n",
    "# #             except ValueError as e:\n",
    "# #                 # Handle errors and append to the results\n",
    "# #                 t_test_results.append({\n",
    "# #                     \"Categorical Variable\": category_dimension,\n",
    "# #                     \"Category\": category,\n",
    "# #                     \"Quantitative Variable\": quant,\n",
    "# #                     \"T-statistic\": None,\n",
    "# #                     \"p-value\": None,\n",
    "# #                     \"Mean\": None,\n",
    "# #                     \"Error\": str(e)\n",
    "# #                 })\n",
    "\n",
    "# # # Convert the results to a DataFrame\n",
    "# # t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# # # Sort results by p-value\n",
    "# # t_test_results_df.sort_values('p-value', inplace=True, ascending=True)\n",
    "\n",
    "# # # Display the results\n",
    "# # t_test_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns='REPORT_DATE', axis=1, inplace=True)\n",
    "# df.to_csv('../../data/log_data_fe_reducedoutliers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns='REPORT_DATE', axis=1, inplace=True)\n",
    "# df.to_csv('../../data/featured_engineered_testing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
