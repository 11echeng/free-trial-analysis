{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions on Test Set:\n",
      "[ 98.11578947 118.83157895 104.78947368]\n",
      "\n",
      "Actual Test Scores:\n",
      "[89 90 95]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class ScoreFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Computes new features based on the target column 'score'.\n",
    "\n",
    "    Features computed:\n",
    "      - lag_score: The previous score for each student and subject\n",
    "      - running_avg: The average of all past scores for each student and subject\n",
    "      - subject_loo_avg: For each subject, the leave-one-out average score\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        # Sort by name, subject, and date so that lag and running average are computed in order\n",
    "        df = df.sort_values(['name', 'subject', 'date']).reset_index(drop=True)\n",
    "\n",
    "        # Compute lag feature\n",
    "        df['lag_score'] = df.groupby(['name', 'subject'])['score'].shift(1)\n",
    "\n",
    "        # Compute running average - fixed version\n",
    "        df['running_avg'] = df.groupby(['name', 'subject'])['score'].transform(\n",
    "            lambda x: x.expanding().mean().shift(1)\n",
    "        )\n",
    "\n",
    "        # Compute leave-one-out average for the subject\n",
    "        def loo_avg(s):\n",
    "            total = s.sum()\n",
    "            count = s.count()\n",
    "            return pd.Series(\n",
    "                [(total - x) / (count - 1) if count > 1 else np.nan for x in s],\n",
    "                index=s.index\n",
    "            )\n",
    "\n",
    "        df['subject_loo_avg'] = df.groupby('subject')['score'].transform(loo_avg)\n",
    "\n",
    "        # Drop the target column\n",
    "        df = df.drop(columns=['score'])\n",
    "        return df\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F', 'M', 'F', 'M'],\n",
    "    'name': ['Alice', 'Bob', 'Alice', 'Bob', 'Alice', 'Bob', 'Alice', 'Bob'],\n",
    "    'score': [85, 90, 88, 92, 87, 89, 90, 95],\n",
    "    'subject': ['Math', 'Math', 'Science', 'Science', 'Math', 'Science', 'Science', 'Science'],\n",
    "    'date': pd.to_datetime([\n",
    "        '2023-01-10', '2023-01-12',\n",
    "        '2023-01-15', '2023-01-16',\n",
    "        '2023-02-10', '2023-02-12',\n",
    "        '2023-02-15', '2023-02-16'\n",
    "    ])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Train/Test Split\n",
    "X_with_score = df.copy()\n",
    "y = X_with_score['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_with_score, y, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "# Date conversion function\n",
    "def convert_date(df):\n",
    "    df = df.copy()\n",
    "    df['date_numeric'] = df['date'].astype(np.int64) // 10**9\n",
    "    return df\n",
    "\n",
    "# Apply date conversion\n",
    "X_train = convert_date(X_train)\n",
    "X_test = convert_date(X_test)\n",
    "\n",
    "# Define features\n",
    "numeric_features = ['lag_score', 'running_avg', 'subject_loo_avg', 'date_numeric']\n",
    "categorical_features = ['gender', 'name', 'subject']\n",
    "\n",
    "# Create preprocessor with imputation\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create and train pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_eng', ScoreFeatureEngineer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions on Test Set:\")\n",
    "print(predictions)\n",
    "print(\"\\nActual Test Scores:\")\n",
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>85</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>90</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>88</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>92</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>87</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>89</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>90</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>95</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender   name  score  subject       date\n",
       "0      F  Alice     85     Math 2023-01-10\n",
       "1      M    Bob     90     Math 2023-01-12\n",
       "2      F  Alice     88  Science 2023-01-15\n",
       "3      M    Bob     92  Science 2023-01-16\n",
       "4      F  Alice     87     Math 2023-02-10\n",
       "5      M    Bob     89  Science 2023-02-12\n",
       "6      F  Alice     90  Science 2023-02-15\n",
       "7      M    Bob     95  Science 2023-02-16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['name', 'subject', 'date']).reset_index(drop=True)\n",
    "df['lag_score'] = df.groupby(['name', 'subject'])['score'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>lag_score</th>\n",
       "      <th>running_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>85</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>87</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>88</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>90</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>90</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>92</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>89</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>95</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender   name  score  subject       date  lag_score  running_avg\n",
       "0      F  Alice     85     Math 2023-01-10        NaN          NaN\n",
       "1      F  Alice     87     Math 2023-02-10       85.0         85.0\n",
       "2      F  Alice     88  Science 2023-01-15        NaN          NaN\n",
       "3      F  Alice     90  Science 2023-02-15       88.0         88.0\n",
       "4      M    Bob     90     Math 2023-01-12        NaN          NaN\n",
       "5      M    Bob     92  Science 2023-01-16        NaN          NaN\n",
       "6      M    Bob     89  Science 2023-02-12       92.0         92.0\n",
       "7      M    Bob     95  Science 2023-02-16       89.0         90.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sort by date within each group\n",
    "df = df.sort_values(['name', 'subject', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Then calculate the running average\n",
    "df['running_avg'] = df.groupby(['name', 'subject'])['score'].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 0    85\n",
      "1    87\n",
      "4    90\n",
      "Name: Math, dtype: int64\n",
      "total: 262\n",
      "count: 3\n",
      "s: 2    88\n",
      "3    90\n",
      "5    92\n",
      "6    89\n",
      "7    95\n",
      "Name: Science, dtype: int64\n",
      "total: 454\n",
      "count: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>lag_score</th>\n",
       "      <th>running_avg</th>\n",
       "      <th>subject_loo_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>85</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>87</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>88</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>90</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>90</td>\n",
       "      <td>Math</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>92</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>89</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>95</td>\n",
       "      <td>Science</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>89.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender   name  score  subject       date  lag_score  running_avg  \\\n",
       "0      F  Alice     85     Math 2023-01-10        NaN          NaN   \n",
       "1      F  Alice     87     Math 2023-02-10       85.0         85.0   \n",
       "2      F  Alice     88  Science 2023-01-15        NaN          NaN   \n",
       "3      F  Alice     90  Science 2023-02-15       88.0         88.0   \n",
       "4      M    Bob     90     Math 2023-01-12        NaN          NaN   \n",
       "5      M    Bob     92  Science 2023-01-16        NaN          NaN   \n",
       "6      M    Bob     89  Science 2023-02-12       92.0         92.0   \n",
       "7      M    Bob     95  Science 2023-02-16       89.0         90.5   \n",
       "\n",
       "   subject_loo_avg  \n",
       "0            88.50  \n",
       "1            87.50  \n",
       "2            91.50  \n",
       "3            91.00  \n",
       "4            86.00  \n",
       "5            90.50  \n",
       "6            91.25  \n",
       "7            89.75  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute leave-one-out average for the subject\n",
    "def loo_avg(s):\n",
    "    print(\"s:\", s)\n",
    "    total = s.sum()\n",
    "    print(\"total:\", total)\n",
    "    count = s.count()\n",
    "    print(\"count:\", count)\n",
    "    return pd.Series(\n",
    "        [(total - x) / (count - 1) if count > 1 else np.nan for x in s],\n",
    "        index=s.index\n",
    "    )\n",
    "\n",
    "df['subject_loo_avg'] = df.groupby('subject')['score'].transform(loo_avg)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 107\u001b[0m\n\u001b[0;32m    100\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    101\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_eng\u001b[39m\u001b[38;5;124m'\u001b[39m, ScoreFeatureEngineer()),\n\u001b[0;32m    102\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m    103\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, LinearRegression())\n\u001b[0;32m    104\u001b[0m ])\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    110\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    674\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    676\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 678\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cheng\\Workspace\\Paramount\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data with 5 columns: gender, name, score, subject, date.\n",
    "data = {\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F', 'M', 'F', 'M'],\n",
    "    'name': ['Alice', 'Bob', 'Alice', 'Bob', 'Alice', 'Bob', 'Alice', 'Bob'],\n",
    "    'score': [85, 90, 88, 92, 87, 89, 90, 95],\n",
    "    'subject': ['Math', 'Math', 'Science', 'Science', 'Math', 'Math', 'Science', 'Science'],\n",
    "    'date': pd.to_datetime([\n",
    "        '2023-01-10', '2023-01-12',\n",
    "        '2023-01-15', '2023-01-16',\n",
    "        '2023-02-10', '2023-02-12',\n",
    "        '2023-02-15', '2023-02-16'\n",
    "    ])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# =============================================================================\n",
    "# Custom Transformer for Feature Engineering\n",
    "# =============================================================================\n",
    "class ScoreFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Computes new features based on the target column 'score'.\n",
    "\n",
    "    Features computed:\n",
    "      - lag_score: The previous score for each student and subject.\n",
    "      - running_avg: The average of all past scores for each student and subject.\n",
    "      - subject_loo_avg: For each subject, the leave-one-out average score (i.e. average score\n",
    "        computed using all other rows in that subject).\n",
    "\n",
    "    After computing these features, the transformer drops the original 'score' column so that\n",
    "    it is not used as an input feature in the model.\n",
    "\n",
    "    Note: In a real-world time-aware scenario, you would need to ensure that only past data are\n",
    "          used to compute these features. Here, we demonstrate a leave-one-out and lag approach.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Nothing special to learn from X; all computations are done per group.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        # Sort by name, subject, and date so that lag and running average are computed in order.\n",
    "        df = df.sort_values(['name', 'subject', 'date']).reset_index(drop=True)\n",
    "\n",
    "        # Compute lag feature: previous score for each student in each subject.\n",
    "        df['lag_score'] = df.groupby(['name', 'subject'])['score'].shift(1)\n",
    "\n",
    "        # Compute running average: average of all previous scores.\n",
    "        df['running_avg'] = df.groupby(['name', 'subject'])['score'].expanding().mean().shift(1)\n",
    "\n",
    "        # Compute leave-one-out average for the subject.\n",
    "        def loo_avg(s):\n",
    "            total = s.sum()\n",
    "            count = s.count()\n",
    "            return s.apply(lambda x: (total - x) / (count - 1) if count > 1 else np.nan)\n",
    "\n",
    "        df['subject_loo_avg'] = df.groupby('subject')['score'].transform(loo_avg)\n",
    "\n",
    "        # Return the DataFrame without the target column.\n",
    "        df = df.drop(columns=['score'])\n",
    "        return df\n",
    "\n",
    "# =============================================================================\n",
    "# Train/Test Split (Simulating Production)\n",
    "# =============================================================================\n",
    "# IMPORTANT: To avoid leakage in a time-aware scenario, you should perform a time-based split.\n",
    "# Here, we simply do a train_test_split with shuffle=False.\n",
    "# Also note: we keep 'score' in X so that our custom transformer can compute features,\n",
    "# but then it drops 'score' so the model does not see it.\n",
    "X_with_score = df.copy()\n",
    "y = X_with_score['score']\n",
    "\n",
    "# For demonstration, we do a time-aware split by not shuffling (assume rows are in order).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_score, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# =============================================================================\n",
    "# Preprocessor: Handling Numeric and Categorical Features\n",
    "# =============================================================================\n",
    "# We need to process the newly engineered features (which are numeric) along with other numeric features.\n",
    "# For the date, we convert it to a numeric value (e.g., timestamp).\n",
    "# (In a real scenario you might extract year/month/day features.)\n",
    "def convert_date(df):\n",
    "    df = df.copy()\n",
    "    df['date_numeric'] = df['date'].astype(np.int64) // 10**9  # seconds since epoch\n",
    "    return df\n",
    "\n",
    "# Apply this conversion to training and test sets.\n",
    "X_train = convert_date(X_train)\n",
    "X_test = convert_date(X_test)\n",
    "\n",
    "# List of numeric features (after feature engineering these will be available)\n",
    "numeric_features = ['lag_score', 'running_avg', 'subject_loo_avg', 'date_numeric']\n",
    "# List of categorical features (we exclude 'date' because we converted it)\n",
    "categorical_features = ['gender', 'name', 'subject']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Build the Pipeline\n",
    "# =============================================================================\n",
    "# The pipeline does the following steps:\n",
    "#  1. Applies the custom feature engineering transformer (which computes lag, running average,\n",
    "#     and leave-one-out features, then drops the target 'score').\n",
    "#  2. Applies the preprocessor (scaling numeric features and one-hot encoding categoricals).\n",
    "#  3. Fits a model (LinearRegression in this example).\n",
    "pipeline = Pipeline([\n",
    "    ('feature_eng', ScoreFeatureEngineer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Train the Pipeline\n",
    "# =============================================================================\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# =============================================================================\n",
    "# Predict on the Test Set\n",
    "# =============================================================================\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions on Test Set:\")\n",
    "print(predictions)\n",
    "print(\"\\nActual Test Scores:\")\n",
    "print(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
